# DeepSeek-Fine-Tuning with Unsloth & Alpaca-GPT4ğŸ”§

![DeepSeek Fine-Tuning](https://img.shields.io/badge/LLM-Fine%20Tuning-blue)
![Python](https://img.shields.io/badge/Python-3.9%2B-green)
![License](https://img.shields.io/badge/License-MIT-orange)

## ğŸ§  Overview

This repository demonstrates instruction fine-tuning of the DeepSeek Large Language Model using the vicgalle/alpaca-gpt4 dataset and Unslothâ€™s FastLanguageModel.from_pretrained for highly efficient training.

The project focuses on fast, memory-optimized LLM fine-tuning, enabling large-model experimentation on limited hardware while maintaining strong instruction-following performance.

---

## ğŸš€ Key Highlights

âš¡ Unsloth FastLanguageModel for optimized model loading

ğŸ§  Instruction fine-tuning with alpaca-gpt4

ğŸ”§ Modular, extensible training pipeline

ğŸ’¾ Reduced VRAM usage and faster training

ğŸ“¦ Clean separation of configs, scripts, core logic, and tests

## ğŸ“˜ Dataset
vicgalle/alpaca-gpt4

A high-quality instruction dataset generated using GPT-4, designed to align open-source LLMs with human-like instruction following.

Sample format:
```json
{
  "instruction": "Explain bias vs variance.",
  "input": "",
  "output": "Bias refers to systematic error..."
}
```

This dataset improves:

Instruction adherence

Reasoning quality

Response clarity and structure

## ğŸ§  Model Loading (Unsloth)

The model is loaded using Unslothâ€™s optimized API, enabling:

Faster initialization

Memory-efficient training

Seamless PEFT / LoRA integration

    ```python
    from unsloth import FastLanguageModel

    def load_deepseek():
        logger.info("ğŸš€ Loading DeepSeek Model")
        return FastLanguageModel.from_pretrained( **model_config()['DeepSeek'] # or use a custom map if needed
        )
    ```

This approach allows fine-tuning large models on consumer-grade GPUs.

## ğŸ—ï¸ Project Structure

    ```text
    DeepSeek-Fine-Tuning/
    â”‚
    â”œâ”€â”€ config/               # Training & model configs
    â”œâ”€â”€ scripts/              # Training and inference scripts
    â”œâ”€â”€ src/                  # Core fine-tuning logic
    â”œâ”€â”€ test/                 # Validation tests
    â”œâ”€â”€ run_pipeline.sh       # End-to-end pipeline execution
    â”œâ”€â”€ README.md
    â””â”€â”€ LICENSE
    ```
## ğŸ› ï¸ Tech Stack

Python

PyTorch

Hugging Face Transformers & Datasets

Unsloth

DeepSeek LLM

CUDA

## âš™ï¸ Setup & Installation

    ```bash
    git clone https://github.com/mohsinraza2999/DeepSeek-Fine-Tuning.git
    cd DeepSeek-Fine-Tuning
    python -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    ```


Requirements

Python 3.9+

NVIDIA GPU (CUDA recommended)

Minimum VRAM depends on sequence length and batch size

## ğŸš€ Training

Run the complete fine-tuning pipeline:

 
    ```bash
    run_pipeline.sh
    ```


This performs:

Dataset loading (alpaca-gpt4)

Optimized model initialization (Unsloth)

Instruction fine-tuning

Checkpoint saving

## ğŸ” Inference
    ```bash
    python scripts/infer.py \ --prompt "Explain gradient descent in simple terms."
    ```


The fine-tuned model produces:

More structured answers

Better reasoning

Improved instruction compliance vs base model

## ğŸ“Š Evaluation

Current evaluation:

Qualitative comparison with base DeepSeek

Instruction-following accuracy

Coherence and relevance of responses

Planned improvements:

Automated benchmarks

Human preference evaluation

Task-specific metrics

## ğŸ“ˆ Results & Observations

âš¡ Faster training compared to standard HF loading

ğŸ’¾ Significant VRAM reduction using Unsloth

ğŸ“ˆ Improved instruction adherence after fine-tuning

ğŸ§  Stable convergence on alpaca-gpt4

ğŸ’¡ Use Cases

Instruction-following AI assistants

Technical and educational Q&A systems

Foundation for RAG-based LLM applications

Cost-efficient enterprise LLM adaptation

## ğŸ”® Future Enhancements

LoRA hyperparameter experiments

RAG integration with vector databases

MLflow / W&B experiment tracking

FastAPI inference service

Quantized deployment (4-bit / 8-bit)

## ğŸ‘¨â€ğŸ’» Author

Mohsin Raza
AI / ML Engineer
ğŸ”— https://github.com/mohsinraza2999